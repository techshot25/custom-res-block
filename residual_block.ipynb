{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Residual Block\n",
    "### By Ali Shannon\n",
    "\n",
    "This simple project shows how you can make a simple residual block by passing parameters to two different branches and then concatenating them into one module and running another layer on them.\n",
    "\n",
    "This is done with pytorch low level API so it might to work in Skorch or Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=100, n_informative=90, n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the network below the inputs are passed to both **x1** and **x2** for different operations then combined as **x** later to run in the last fully connected layer to softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Linear(100, 75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(75, 50),\n",
    "        )\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Linear(100, 50),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(100, 3)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        # first branch\n",
    "        x1 = self.branch1(x)\n",
    "        \n",
    "        # second branch\n",
    "        x2 = self.branch2(x)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "model = Net().cuda().double() # I use CUDA because I have a gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (branch1): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=75, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=75, out_features=50, bias=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (1): Dropout(p=0.2)\n",
      "  )\n",
      "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(X).cuda()\n",
    "y = torch.tensor(y).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 \tLoss: 0.960346\n",
      "epoch 200 \tLoss: 0.859943\n",
      "epoch 300 \tLoss: 0.805659\n",
      "epoch 400 \tLoss: 0.767474\n",
      "epoch 500 \tLoss: 0.722628\n",
      "epoch 600 \tLoss: 0.702597\n",
      "epoch 700 \tLoss: 0.682851\n",
      "epoch 800 \tLoss: 0.654043\n",
      "epoch 900 \tLoss: 0.643702\n",
      "epoch 1000 \tLoss: 0.632673\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1001):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch} \\tLoss: {loss.item():.6f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the residual block net is 96.2%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_pred = model(x).detach().cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(np.argmax(y_pred, axis = 1), y.cpu().numpy())\n",
    "\n",
    "print(f'Accuracy of the residual block net is {acc*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
