{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Residual Block\n",
    "### By Ali Shannon\n",
    "\n",
    "This simple project shows how you can make a simple residual block by passing parameters to two different branches and then concatenating them into one module and running another layer on them.\n",
    "\n",
    "This is done with pytorch low level API so it might to work in Skorch or Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=10000, n_features=100, n_informative=90, n_classes=10)\n",
    "\n",
    "X = torch.from_numpy(X).cuda()\n",
    "y = torch.from_numpy(y).cuda()\n",
    "\n",
    "X_train, y_train = X[:-100], y[:-100]\n",
    "X_test, y_test = X[-100:], y[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the network below the inputs are passed to both **x1** and **x2** for different operations then combined as **x** later to run in the last fully connected layer to softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(num_features = 100, momentum = 0.999)\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Linear(100, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "                \n",
    "        self.fc = nn.Linear(256, 10)\n",
    "        \n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.bn(x)\n",
    "        \n",
    "        # first branch\n",
    "        x1 = self.branch1(x)\n",
    "        \n",
    "        # second branch\n",
    "        x2 = self.branch2(x)\n",
    "        \n",
    "        #x = torch.add(x1, x2)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "model = Net().double().cuda() # cuda for for devices with Nvidia GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (bn): BatchNorm1d(100, eps=1e-05, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  (branch1): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.2)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): Dropout(p=0.2)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (7): Dropout(p=0.2)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
      "    (1): Dropout(p=0.2)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 1.5868\tValidation Loss: 1.6570\n",
      "Epoch: 20 \tTraining Loss: 1.5098\tValidation Loss: 1.5889\n",
      "Epoch: 30 \tTraining Loss: 1.4951\tValidation Loss: 1.5574\n",
      "Epoch: 40 \tTraining Loss: 1.4901\tValidation Loss: 1.4972\n",
      "Epoch: 50 \tTraining Loss: 1.4856\tValidation Loss: 1.5563\n",
      "Epoch: 60 \tTraining Loss: 1.4821\tValidation Loss: 1.5339\n",
      "Epoch: 70 \tTraining Loss: 1.4842\tValidation Loss: 1.5560\n",
      "Epoch: 80 \tTraining Loss: 1.4853\tValidation Loss: 1.5401\n",
      "Epoch: 90 \tTraining Loss: 1.4831\tValidation Loss: 1.5438\n",
      "Epoch: 100 \tTraining Loss: 1.4849\tValidation Loss: 1.5475\n"
     ]
    }
   ],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "        \n",
    "for epoch in range(1, 101):\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    # train the network\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size = 1000):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    running_loss *= len(X_batch)/len(X_train)\n",
    "    \n",
    "    # validate with testing data\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(X_test)\n",
    "    loss = criterion(y_pred, y_test)\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} \\tTraining Loss: {running_loss:.4f}\\tValidation Loss: {loss.item():.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the residual block net is 91.0%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_pred = model(X_test).detach().cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(np.argmax(y_pred, axis = 1), y_test.cpu().numpy())\n",
    "\n",
    "print(f'Accuracy of the residual block net is {acc*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
